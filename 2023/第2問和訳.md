## 要旨

大規模な事前学習は、GPT-3、Megatron-Turing NLG、Gopher などのような、能力のある汎用的な生成モデルの作成技術として最近注目されてきました。本論文では、このようなモデルの逆説的な性質を強調し、その性質に関する政策上の意義について議論します。具体的には、これらの生成モデルは広範な訓練分布における予測可能な損失（「スケーリング則」に具体化される）と、予測できない特定の能力、入出力の組み合わせを持つという逆説的な組み合わせを有しています。高いレベルでの予測可能性と有用な能力の外見が、このようなモデルの迅速な開発を推進している一方、予測できない特性はモデルの展開の結果を予測することを難しくしています。我々は、この組み合わせがどのようにして文献や実世界の観察から社会的に有害な行動を引き起こす可能性があるかについて例を挙げながら説明し、また予測の困難さから生じる実験を 2 つ行って、この点についての私たちの意見を説明します。さらに、これらの相反する性質がモデル開発者にモデルを展開するさまざまな動機と、展開を妨げる可能性のある課題をどのようにもたらすかについて分析します。最後に、AI コミュニティがこれらのモデルが有益な影響を持つ可能性を高めるために取る可能性のある介入策のリストを示します。本論文は、AI システムを理解し規制することを望む政策立案者、自身の作業の潜在的な政策への影響を気にする技術者、これらの課題に取り組む作業を支援したい支援者、そして大規模生成モデルを分析し批評し、可能性を検討する学者にとって有益なものとなることを意図しています。

## 1.序論

ニューラルネットワークのデータ量、計算能力、モデルパラメータのスケーリングアップは、最近、CLIP [ 62 ]、Ernie 3.0 Titan [ 82 ]、FLAN [ 83 ]、Gopher [ 63 ]、GPT-3 [ 12 ]、HyperClova [ 47 ]、Jurassic-1-Jumbo [ 52 ]、Megatron Turing NLG [ 73 ]、LaMDA [ 79 ]、Pan Gu [ 90 ]、Yuan 1.0 [ 88 ]などの能力のある生成モデルの登場（および実世界での展開）につながりました。このクラスのモデル 1 において、スケールとモデルの性能との関係は、しばしば法則的な関係で説明できるほど予測可能であり、スケーリング則として説明できます。ほとんどの場合、これらのスケーリング則は、モデルが大きくなるにつれて特定の能力が継続的に向上することを予測します。同時に、大きな生成モデルは機械学習における注目すべき結果の増加する割合を表すようになっています。その結果、多くの機関が過去数年間に大規模なモデルを製造し始めており、スケーリング則によって提供される予測可能性と、これらのモデルを検索エンジンのような経済的価値を生成するシステムに組み込むことができる事実に応じています。また、これらのモデルが新たな有害な行動のリスクを示すことも明らかになっており、これらのリスクは予測が難しく、モデルの能力が向上するにつれてより深刻になる可能性があります。より小さなモデルでこれらの危害を研究しようとする試みは、大きなモデルで発生する事象を正確に反映しないかもしれません。
本論文では、大規模モデルの開発と展開のダイナミクスにおけるスケーリング則の影響をより良く理解し、特に大規模言語モデルに焦点を当てて説明します。我々の基本的なテーゼは、大規模な生成モデルが高い予測可能性と高い予測不可能性という逆説的な組み合わせを持っているということです。モデルの損失はトレーニングに費やされたリソースに応じて改善し、多くのタスクにおけるパフォーマンスの向上とは緩く相関する傾向があるため、前者はモデルの急速な開発を推進します。一方、特定のモデルの能力、入力、および出力は事前に予測することができないため、後者はこれらの開発と展開の結果を予測することが難しくなります。我々は、この組み合わせがどのようにして社会的に有害な行動につながる可能性があるかについての例を示しながら、この問題に取り組むモデル開発者が直面する動機と課題も分析します。本論文の目標は、これらのモデルの開発がどのようにして行われるか、なぜ行われるかを概説し、モデルの開発を指導するための介入策を特定することです。我々は、セクション 2 で述べた課題を踏まえて、大規模モデルの展開の安全性を高め、これらのモデルを構築する開発者のインセンティブ構造を改善するためのいくつかの政策提言をまとめます。スケーリング則、オープンエンド性、大規模モデルの普及に関するすべての個別のポイントは、明示的または暗黙的に他の研究で提示されているものですが、ここでの私たちの貢献は、その意義を含めて完全な画像を強調することです。
スケーリング則に焦点を当てていますが、私たちの多くのポイントは、大規模なモデルの展開に伴う社会的リスクに関する関連する研究を補完しています[8、10、22、45、76、84]。ただし、[84]と同様に、トレーニングデータの作成と注釈付けに関与する人的労力のコスト[31]、モデルのトレーニングに使用するための必要なハードウェアの供給チェーンの倫理[20]、モデルのトレーニングの環境への影響[8、57、70、75]については考慮していません。スケーリング則は、これらの問題に大きな影響を及ぼす可能性があります。

本論文の残りの部分は次のように構成されています。セクション 2 では、大規模生成モデルに関する私たちの中心的な論点を述べ、それを 4 つの主張に分解して支持します。各主張は、以前に公開されたデータからの証拠、および一部のケースでは大規模言語モデル[4]での新しい実験によって支持されます。セクション 2.1 では、スムーズな一般的な能力のスケーリングについて議論します。具体的には、一般的な能力のスケーリングにおいて、2 つのことを意味します。第一に、幅広いデータ分布において、トレーニング（およびテスト）の損失がスケールに応じて予測可能に改善すること。第二に、この損失の改善は、平均して多くの下流タスクでの性能向上と相関する傾向がある[12、63]。私たちは、これら 2 つの特性の組み合わせを、本論文全体を通じてスムーズな一般的な能力のスケーリングと呼んでいます。セクション 2.2 では、急激な特定の能力のスケーリングについて説明します。この場合、モデルはスケールにおいて急に特定の能力を獲得することもあります。我々は文献[5、12、63]からの 3 つの例を用いて、この現象を説明します。セクション 2.3 では、モデルの能力の領域全体は、特定の入力、問題領域、アプリケーションから要求されるまで不明である可能性があると主張します。セクション 2.4 では、モデルの出力のオープンエンド性から生じる課題について議論し、スケールに伴って有害で有毒な出力が現れる例を定性的および定量的に示します。

セクション 3 では、予測可能性と予測不可能性というこれらの相反する特性にもかかわらず、セクション 2 で概説した課題にもかかわらず、大規模生成モデルの開発と展開が増加すると予想される理由について概説します。これは、経済的、科学的、および名声の動機の融合によるものであり、それぞれを要約します。また、開発者が開発および展開中に直面する可能性のあるいくつかの障壁についても考慮します。これには、高い財務コスト、エンジニアリングの才能へのアクセス、安全上の懸念、有能な生成モデルを責任を持って展開する方法の標準の欠如などが含まれます。また、大規模言語モデルの開発がこれまでどのように進展してきたかについても、上記で説明した動機と課題に基づく実証的な観察（学術界と産業界の大規模モデル開発の間のギャップの増加に関する定量的分析を含む）を提供します。

最後に、セクション 4 では、セクション 2 と 3 で概説した課題に具体的に対処するための政策介入について概説し、広範な社会的利益のためにより大きなモデルの開発と展開を導くための指針を提供します。私たちは、いくつかの説明的な実験、技術的な詳細、および主張に関する注意事項を付録 A に記載しています。

## 2 大規模生成モデルの特徴

我々は、大規模生成モデル（例：GPT-3 [12]、LaMDA [79]、Gopher [63]など）が次の 4 つの特徴によって特徴づけられると主張します：

• 滑らかな一般的な能力のスケーリング：モデルのサイズ、トレーニングに使用する計算、およびトレーニングに使用するデータ量を正しい比率でスケーリングすることによって、生成モデルの一般的なパフォーマンス（特定の広範なデータ分布に対する損失）を予測可能に改善することができます。これらの比率はスケーリング則によって正確に予測することができます（図 1）。これらのスケーリング則により、リソースコストが高く、モデルが特定のタスクでどのようにパフォーマンスするかを正確に予測することが難しいにも関わらず、より大きく一般的な能力を持つモデルを構築するための投資のリスクが軽減されます。注意すべきは、毒性などのモデルの有害な特性も直接的に有益な能力と並行してスケーリングする可能性があることです。

• 急激な特定の能力のスケーリング：一般的なレベルではパフォーマンスが予測可能であるにもかかわらず、特定のタスクにおけるパフォーマンスは時折、スケールで予測不可能かつ急激に現れることがあります。これは、特定のタスクがモデルの出力確率分布のごく一部であるため、全体の分布が滑らかである限り、急速に変化する可能性があるためです。

• オープンエンドな入力とドメイン：大規模生成モデルはオープンエンドであり、任意のドメインに関するさまざまな範囲の入力を受け入れることができます。その結果、特定の能力（あるいは能力全体）は、その知識を引き出すための入力が提供されるまで不明です。モデルがトレーニングされた後でも、作成者やユーザーはその（有害な可能性があるかもしれない）能力のほとんどを認識していない可能性があります。これらの特性はモデルがスケールするにつれてより顕著になり、大きなモデルほど小さなモデルよりも特徴付けが難しくなる傾向があります。

• オープンエンドな出力：最後に、モデルの出力もオープンエンドであり、固定されたスケール、入力、トピック、またはタスクでも予測や制御が難しいです。これらの出力は有益な場合も有害な場合もありますが、事前に予測するのは難しいです。もちろん、オープンエンドな入力と出力を持つモデルは数十年前から存在していますが、新しいのは能力のレベルとオープンエンド性の広がりです。

次のセクションでは、これらの特徴のそれぞれをさらに詳しく説明し、それらの組み合わせがどのようにして社会的な影響をもたらす可能性があるかについて議論します。我々の主張はデータや実験で裏付けられており、それらを支持します。特にスムーズな一般的なスケーリングがタスクのパフォーマンスとどのように相関し、スケールに基づいた分析が特定のモデルの潜在的な経済的価値を予測するためにどのように使用できるかを示すために、スケーリング則に基づいた分析の小規模な実験を Appendix A.3 に記載します。

### 2.1 スムーズな一般的な能力スケーリング

一般的に、機械学習の実験は正確に予測できるわけではありません。複雑なモデルを複雑なデータで訓練すると、ノイズや変動のある結果が得られることが一般的です [19, 91]。ただし、個々の実験が予測できないとしても、大規模生成モデルの一般的な性能は、スケールの関数としてスムーズかつ予測可能な成長を示す傾向があります。これは、モデルのサイズが増加すると、機械翻訳や音声認識などの機能がスムーズかつ予測可能な方法で向上することを最初に気付いた[40]によって最初に気付かれました。その後の研究は、スケール（モデルのサイズとトレーニングデータのサイズの両方で）とモデルの一般化誤差の量的な関係を形式化し、実験的に検証しました[67]。さらに、[44]は、言語モデリングタスクのテスト損失パフォーマンスが、モデルのサイズ、データセットのサイズ、およびトレーニングの期間の予測可能な関数としてスケーリングすることを示しました。これらの三つの要因は、化学反応の材料のようなものであり、すべてが同時にスケールアップされる場合、テスト損失が比例して改善されます。ただし、1 つの材料が不足していると、その材料によって制限されます。これらのトレンドは、わずかな偏差しかなく、データの数十のデータポイントといくつかの桁をカバーする、非常に単純なデータに合ったフィットからほとんど外れていません（図 1）。その後の研究は、他のモダリティ（画像、ビデオ、数学など）に対する生成モデル、オーディション[24]、テキストからプログラミングへの転送[39]、ビジョンモデルのフューション適応[61]などのスケーリング法則が存在することを示しています。

予測可能なスケーリング、特にデータ、モデルサイズ、およびトレーニングの正確な組み合わせに対する依存性は、モデルの開発プロセスに影響を与えます。これにより、このタイプのモデルの開発は、職人技の試行錯誤のプロセスから、より予測可能なエンジニアリングプロセスへとシフトします。特定の結果を達成するために必要なリソースは正確に計算でき、それらのリソースのコストを結果の有用性と比較することができるようになります。非常に具体的な動作は予測できないかもしれませんが（セクション 2.2 で詳細に説明します）、一般的なテスト損失は平均して多くのタスクとよく相関する傾向があり、したがって大きなモデルは通常、全般的に大きな進歩を遂げます。この意味で、スケーリング則は大規模モデルへの投資のリスクを軽減します。セクション 3.1 でこれについて詳しく説明し、開発者がスケーリング則をどのように利用できるかに関する技術的な詳細を付録 A.2 に提供します。スムーズな一般的なスケーリングがタスクのパフォーマンスとどのように相関するかをさらに説明し、スケールベースの分析を使用して特定のモデルの潜在的な経済的価値を予測する方法について説明します。付録 A.3 には、スケールと GPT-3 のような言語モデル[4]との関係を分析する小さな独自の実験の概要が記載されており、これはゼロショット学習を用いた推薦システムとして使用するためのものです。この例を選んだのは、推薦システムが具体的な経済的関連性と社会的影響があり、特定のドメインのアルゴリズムによって機械学習でよく研究されている[34]が、通常は大規模生成モデルで研究されることはないからです。驚くべきことに、我々は、大規模化に従って生成モデルが最小限の努力と極めて限られた明示的なトレーニングデータへのアクセスで、ますます簡単な推薦システムとして機能することを発見しました。詳細な分析と議論は付録 A.3 に残しています。

### 2.2 急激な特定の能力のスケーリング

モデルのサイズによる広範なタスクのパフォーマンスは滑らかにスケーリングするかもしれませんが、質的に異なる特定の能力は急激で不連続に現れることがあります。この現象がいつ、またなぜ起こるのかは明確ではありません。しかし、直感的に特定の能力の急激なスケーリングは、日常の天気が季節平均よりも予測が難しいのと同じ理由で共存する可能性があります：個々のデータポイントは広い平均よりもはるかに変動する可能性があります。
ここでは、算術 [12]、言語理解 [35, 63]、プログラミング [5] の 3 つの例を挙げて、急激な能力のスケーリングを説明します（図 2）。算術の場合、GPT-3 は、演算および桁数によって異なりますが、6B のパラメータと 175B のパラメータの間のどこかで急激な能力の転換が現れます [12]。例えば、3 桁の加算は 6B のパラメータ未満のどのモデルでも正確に実行される確率は 1%未満ですが、13B のパラメータのモデルでは正確さが 8%に跳ね上がり、175B のパラメータのモデルでは 80%の正確さを達成します。これにより、「ホッケースティック」のようなグラフ（図 2、左）が生じ、算術の能力が何もなかった数桁の後に突然現れる現象が起こります。
異なる言語モデルである DeepMind の Gopher [63]も、異なるデータセットである MMLU 言語理解ベンチマーク [35] において急激なパフォーマンスのジャンプを示しています（図 2、中央、オレンジ）。6B のパラメータ未満のすべてのモデルに対して、Gopher の精度は 30%未満であり、これはわずかにチャンス（25%の正答率）を上回っています。しかし、280B のパラメータを持つ Gopher モデルは 60%の正答率を達成し、大きな飛躍です。GPT-3 も同様の現象を示していますが、その影響は小さいです（図 2、中央、青）。
第三の例として、Google から提供されている最近のプログラム合成モデルは、サイズが 10B から 100B のパラメータに増加するにつれて、コンピュータプログラムを作成する能力が劇的に向上します [5]（図 2、右）。例えば、与えられたプログラミングの問題を解決する合成プログラムの割合は、モデルサイズが 68B から 138B のパラメータにおいて 2 倍増加すると、6%から 13%に大幅に増加します。急激な特定の能力のスケーリングは、大規模モデルの安全保証と展開において重要な課題を提起します。私たちは比較的無害な能力についてこの現象を示しましたが、スケールにおいて（小さなモデルには存在しない）潜在的に有害な能力が現れ、予測が困難な可能性があるかもしれません。これについては、次のセクションで詳しく探求します。

### 2.3 オープンエンドな入力とドメイン

大規模生成モデルはオープンエンドであり、さまざまなドメインから任意の入力を受け入れ、（しばしば関連性があり創造的な）出力を生成します。その結果、モデルの振る舞いの一部は特定の入力から引き出されるまで不明です。また、事前にトレーニングされた生成モデルは新しいデータに対して微調整され、新しい問題を解決するために使用できるようになります。このような微調整を広範に有効にすることは、モデルの能力の広がりと、モデルの振る舞いを予測したり制約したりする難しさを大幅に増加させます。このオープンエンド性は、AI の開発者がテストされていない入力に対して予期しない（そして可能性のある有害な）振る舞いを完全に理解せずにシステムを展開する可能性があるため、難しいものです。
例えば、[AI Dungeon](https://beta.aidungeon.com/) というビデオゲームでは、ファンタジーの役割を演じるために GPT-3 を微調整しました [77]。しかし、適切な入力を使うことで、プレイヤーは任意のトピックについて話すように操作することができ、実質的には GPT-3 への一般的なバックドアアクセスを提供してしまいました [56][そのほかの議論](https://www.reddit.com/r/AIDungeon/comments/i2shib/ai_dungeon_creator_states_how_ai_dungeon_tries_to/)。したがって、一つの目的のために設計されたユースケースが、実際には GPT-3 の全ての能力を含み、オープンエンドのインターフェースを巧妙に活用することで利用されていました。(訳註：AI Dungeon を普通の GPT-3API みたいに使えた（「文章要約して」、とか）)

オープンエンドの入力とドメインの固有の課題についての私たちの指摘を更に具体的にするため、そして言語モデルからの潜在的な害の可能性に関連付けるために、我々は通常（まだ）適用されていないが、社会的な懸念が関連する問題領域を考えてみましょう：[再犯予測](https://www.science.org/doi/10.1126/sciadv.aao5580)です。一部の人々は、公平性に関する特定の懸念を超えて、再犯予測は単に機械学習のタスクではないと指摘しています[7]。私たちは同意し、言語モデルは再犯予測に使用すべきではないと考えています。しかし、そのような応用がそもそも疑問視されるほど本質的に問題のある場合、それはどのように有害な能力が生成モデルの拡大とともに静かに予期せぬ方法で現れるかの説得力のある例を提供します。そのような急激な出現は、害がより微妙な状況である多くの他の文脈でも起こる可能性が高いです。我々は問題が顕著であるケースを研究し、私たちの論文を明確に示すための事例を検討します。

これを行うために、ProPublica の COMPAS データセットを活用しています。このデータセットには、フロリダ州ブロワード郡で逮捕された 7,000 人以上の被告に関するデータが含まれています[3, 7]。データセットには、被告に関する特徴に基づいて評価から 2 年以内に被告が軽犯罪または重犯罪を犯すリスクを反映するように設計された COMPAS アルゴリズムによって計算された再犯リスクスコアが含まれており、人種を含まないで被告についてのセットに基づいています。さらに、各被告人の実際の再犯結果も含まれています。ProPublica は、これらのリスクスコアは不正確で人種的偏見があると結論づけました[3]。さらなる研究では、刑事司法経験が限られているか全くない人々も、被告人を記述する簡単なプロンプトに基づいて再犯予測を行う際に、COMPAS と同様の不正確さと人種的偏見を示すことがわかりました[23]。人間の被験者実験は、被告人の人種がプロンプトから除外された条件と含まれた条件の両方を調査しました[9]。ここでは、私たちは[23]で概説された同じプロンプトを使用しますが、被告の再犯を予測するために人々の代わりに言語モデル[4]に尋ねます。完全な技術的詳細と（重要な）注意事項は付録 A.4 に記載していますが、ここで重要なのは、COMPAS のようなベンチマークリスク評価機器のデータセットには、多くの測定バイアスとエラーが含まれており、それらが使用される複雑な社会技術システム（この場合、米国の刑事司法システム）を注意深く考慮しない限り、実世界の影響に関する主張をするのに適していないことです[7]。

私たちは、言語モデルが COMPAS と同様の（またはそれ以上の）不正確さと人種的偏見を示すことを発見しました。図 3 は、プロパブリカの分析[3]とその後の人間被験者実験[23]で言及されている 2 つの指標に関して、言語モデルと COMPAS を比較したものです。全体的な予測の正確さと黒人と白人の被告の偽陽性率の比率です。私たちは個人の人種を除外するプロンプト（青）と含むプロンプト（オレンジ）の両方の結果を示しています。全体的な予測の正確さに関しては、言語モデルは、サイズが大きくなるにつれて被告が再犯するかどうかを予測する能力が増すことが分かります（図 3、左）。ただし、COMPAS と同様に、まだ信頼性のない予測モデルです。人種がプロンプトから除外された場合と含まれた場合、予測の正確さにはほとんど差がありません。どちらの条件でも、パラメータ数が 52B の最大モデルは、COMPAS の 66%の正確さに対して 63%の正確さを達成します。

黒人被告と白人被告の間の誤陽性率の比率も高くなっていることが分かります（図 3、右）、これは部分的に[3]で説明されている COMPAS アルゴリズムの人種的偏見を反映しています。COMPAS の場合、この比率は 1.92 であり、実際には再犯しなかったにもかかわらず、黒人被告が白人被告のほぼ 2 倍の頻度で再犯すると予測されています（公平なアルゴリズムでは誤陽性率比は 1 になります）。言語モデルがサイズを増やすにつれて、12B のパラメータ付近で誤陽性率の比率が滑らかに増加し、最大モデルでは、プロンプトから人種が除外された場合には 1.5 の値に達し、プロンプトに人種が含まれる場合には 2.21 の値に達します。後者の場合、最大の言語モデルは COMPAS よりもさらに公平でないです。おそらく、モデルは、COMPAS データセットのごく一部にある人種的バイアスと、事前学習された言語モデルの環境的な人種的バイアスの組み合わせに反応しているのでしょう。

先に述べたことを再度強調しますが、ここでのポイントは、再犯予測タスクにおける人種的バイアスの出現だけでなく、このタスクを実行する能力の出現です。言語モデルがスケールアップするにつれて、多くの人々が本質的に有害であると主張してきたタスクを実行する能力を獲得し、しかもバイアスのかかった方法で実行します[7]。大規模な言語モデルには、これらの問題のいずれかまたは両方を引き起こす（現在まだ発見されていない可能性のある）「スキル」が多く存在する可能性が高いですが、おそらくそれはより明らかでない形で現れるでしょう。

要約すると、事前学習された言語モデルは、その作成者によって予期されていない目的に対して、最小限の努力で適応できる可能性があります。それがモデルの固有の能力を利用してセキュリティ制約を回避する場合（AI Dungeon の例のように）、または新しい能力を新しい入力を通じて発見する場合（セクション 2.2 での能力の急激なジャンプの議論や上記の再犯実験のように）です。また、最も驚くべき能力の多くは大規模な規模で現れることにも注意しており、より小さなモデルで作業することは、そのような能力を探求するのをより難しくするでしょう。

### 2.4 オープンエンドの出力

前のセクションでは、言語モデルがオープンエンドの入力を持つと主張し、これによって予期せぬ能力が現れる可能性があると述べました。しかし、入力やトピックが固定されている場合でも、生成される出力は多様で予測困難なものになることがあります。この種の予測不可能性は、おそらく前述の種類よりもより一般的で広く研究されていると言えるでしょうが、大規模なモデルの振る舞いに複雑さの追加レイヤーをもたらすため、簡単に説明する価値があります。

例として、図 4 では主張を示すために AI アシスタント[4]に対して攻撃的な内容を生成するように依頼しています。比較的明確な入力でモデルにプロンプトを与えたにもかかわらず、モデルは問題の質問とは関連しない出力を生成しています。応答は直接的に攻撃的ではなく、代わりに他の AI システムが犯した違反のリストです。このオープンエンド性の一つの効果は、予測不可能なモデルの応答が人の元のクエリから注意をそらす可能性があることです。
オープンエンド性はまた、事実に関する不正確性の第二かつより有害なリスクを導入します。図 4 のやり取りを詳しく見ると、モデルがこれらの違反事例を捏造していることが分かります。IBM Watson や Microsoft の Tay[87]のようなシステムは展開中に問題を抱えていましたが、AI アシスタントは Watson の場合に年とエラーを誤り、Tay の場合にエラーを（年は正しい）誤っています。これらの例が正しいとモデルに確信を持って尋ねると、モデルは誤解を招く回答をし、質問をする人の権威を疑問視します。これは、特定の入力（例えば、モデルに攻撃的なことを言わせるように要求する）であっても、AI モデルが、単に注意をそらすだけでなく、潜在的に誤導する出力を生成することを示しています。

オープンエンドのモデルの出力は、有害または望ましくないテキストを導入する可能性もあります。例えば、図 5 は、言語モデル[4]から生成されたテキストの有毒性（無礼、非礼、または非合理な言葉遣い[30]）11 がモデルのサイズとともに滑らかにかつ著しく増加することを示しています。最近の研究でも、同様のモデルを使用して異なる分析を行った際に、モデルサイズとともに非常に類似した有毒性の傾向が観察されており（[63]）、これは一般的な現象かもしれません。詳細な内容や注意事項は付録 A.6 に載せました。

多くの応用が、チャットボット、検索エンジン、テキスト要約システム、質問応答システム、機械翻訳システムなどの言語モデルに関して、オープンエンドのテキスト生成に依存しています。そのようなことから、オープンエンドのテキスト生成の社会的に関連する側面 - 関連性、正確性、安全性、そして創造的な表現さえ（AI による詩の生成に関する議論については付録 A5 を参照） - がモデルのサイズとともにどのように変化するかを定量化することは重要です。また、AI モデルの結果の事実の正確性を向上させる技術を開発することも重要であり、例えば[11]に記載されているような手法を含めて、モデルの出力を適切にし、有害なバイアスが表示されにくいようにすることも重要です[74]。

## 3 大規模モデルの開発と展開の動機と課題

前のセクションで、大規模生成モデルは、予測可能な一般的な性能と予測不可能な特定の能力、入力、および出力という 4 つの特徴のパラドックス的な組み合わせを持つという基本的な論点を説明しました。予測可能な一般的な性能と印象的な特定の能力が結びつき、このようなモデルの迅速な開発を推進していますが、予測不可能性がモデルの開発者が展開の結果を予測するのを困難にします。これらの特徴により、大規模生成モデルの開発と展開には多くの動機（および障壁）があります。ここでは、この基本的な緊張関係の要素に焦点を当て、いくつかの経験的な観察を元に議論を行います。
具体的には、セクション 3.1 では、大規模生成モデルを開発し展開する 3 つの顕著な動機を概説します：経済的、科学的、名声的な動機です。逆に、セクション 3.2 では、モデルをスケールさせるために必要な財政的コストとエンジニアリングの専門知識、AI の安全性の問題、モデルの展開に関する標準と規範の欠如に関する 3 つの障壁を概説します。これらの動機と障壁は完全なものではなく、また明らかかもしれません。しかし、これらの要因の組み合わせが、セクション 3.3 で説明するように、現在までの言語モデルの開発と展開がどのように進行してきたのかについてのいくつかの経験的観察を説明するのに値すると考えています。特に、大規模言語モデルが急速に増加しており、産業界と学界の間でこのようなモデルを開発するためのギャップが広がっており、モデルの展開が害や論争を引き起こす事例が多数報告されていることに言及します。

### 3.1 大規模モデルを開発し展開する動機

経済的な動機。モデルの開発に対する最も単純で明白な動機は、経済的なものです。スケーリング法則により、モデルを開発するコストを正確に見積もることができ、経済的な価値のあるアウトプットが損失とスムーズにスケーリングする場合、モデルの訓練に対するリターンも計算できます。これは一般的にも特定にも適用されます。一部の機関は、特定のモデルの能力を広範に向上させたいと考え、そのために経済的なインセンティブを持ちます。一方、他の機関は特定のモデルの能力をターゲットにしており、それにはスケーリング法則が伴うため、それを開発するインセンティブも持ちます。これにより、大規模モデルの訓練のリスクが軽減されます。予測可能な額を投資して比較的予測可能なリターンを得ることができ、不確実なリターンの見込みがある多くの推測的な研究プロジェクトとは異なります。予測可能性は、研究投資の論理をより明確にし、大規模な機関内で正当化するのに役立つ可能性があります。したがって、経済的な動機と続く滑らかな一般的な能力のスケーリングは、モデルの展開が増加すると予想される理由です。特定の AI モデルがどの検索クエリに特に適しているか、どのアプリケーションが成功し、どのアプリケーションが予測できないほど失敗するか、どの開発ワークフローがコード合成モデルを活用して助けられるかを事前に正確に予測することはできないかもしれませんが、これらのアプリケーションはすべて、経済的なリターンをスムーズな一般的な能力のスケーリングに結び付けるための広範な平均を活用しています。

科学的な動機。大規模生成モデルは、言語学やロボティクスから哲学や社会科学まで、幅広い学際的な AI 研究の基盤となる可能性があります。大規模モデルの開発（または少なくともアクセス）がない場合、これらのモデルが健康保険、教育、法律などの社会的に影響力のある研究領域でどのように進歩を促進するかを研究することは困難です。大規模モデルはまた、次世代のアルゴリズムとアーキテクチャを開発するための肥沃なテストベッドです。新しいアルゴリズムは、スケーリング法則をより計算機、データ、またはパラメータの効率的な方向に変えるかどうかに基づいて厳密に評価できます。

名声の動機。これらのモデルが可能性のフロンティアにあるという事実は、それ自体がモデルを開発する名声のインセンティブを作り出します。大規模モデルは、機関の能力を広告する手段となる場合があります。一般の視線で認識される優位性を得る方法、熟練した AI 研究者の採用を容易にする方法、大規模モデルとは無関係のサービスの売上を増やす方法、または国のイニシアティブや国民的な誇りを支援する方法です。これらの動機は、これらのモデルの能力の全体的な範囲についての高い不確実性があるにもかかわらず、大規模生成モデルを開発、公開、展開するための強力なインセンティブを作り出す可能性があります。

### 3.2 大規模モデルを開発し展開する際の障壁

財政的なコストとエンジニアリングの才能。大規模生成モデルをスケーリングアップするには、かなりの財政投資が必要です。例えば、GPT-3 の訓練には数百万ドルかかると推定されています。また、大規模生成モデルをスケーリングアップするには、分散システムエンジニアリング、Kubernetes のようなクラスタ管理ツールの知識、低レベルの GPU プログラミング、継続的な統合テストの管理など、特定のエンジニアリングの能力が必要です。これらのモデルのサイズのため、過去 10 年間に比べて開発のタイムラインが長くなり、より複雑なワークフローが必要になっています。例えば、約 10 年前、その時点で比較的大規模な AI モデルであった AlexNet12 は、2 つの GPU を搭載した単一のデスクトップマシンで数千ドルかかる予算で卒業生が訓練したものでした。

安全性とバイアス。セクション 2 で説明したように、オープンエンド性はスムーズな一般的な能力のスケーリングと特定の能力の急激なスケーリングが組み合わさることで、モデルの開発と展開後に発見される可能性のある安全性の問題[10, 84]を引き起こす可能性が高いです。さらに、これらのモデルは、事前の展開段階で解決策が不十分な既知の安全性の問題も持っています[36]（例：システムが適切でないや有害な出力、たとえば露骨な性差別的または人種差別的なコメントを生成しないようにする方法は？[74] システムがバイアスの問題を特定してデプロイ前に対処する方法は？[9, 60] モデルが主張を出力する際に事実を作り上げていないことをどのように保証するか？[11]など）。

基準と規範の不足。これらの大規模生成モデルは非常に最近（過去 5 年以内）に開発され、経済的な観点から展開する価値があるとされるようになってきたため、これらのシステムを安全に展開するための基準は存在しません。この基準の不足は、セクション 2 で特定した生成モデルの 4 つの特徴および上記で議論した安全性の問題をさらに悪化させます。同時に、これらのモデルの弱点を特定し、それに関連する開発および展開の問題を識別することに重点を置いた研究が増えています[8, 10, 22, 45, 57, 70, 75, 76, 84]。しかし、この研究はまだ開発者が採用できる形式の繰り返し可能な基準として具体化されているわけではなく、モデルの機能、欠点、およびその他の重要な詳細を文書化するためのモデルカード[54]やデータシート[29]を使用して、開発者が自身の展開ポリシーを決定する必要があるため、展開がより困難になります。また、展開は共有された知識が何が「安全な」展開であるかについては少ないため、 inherently risky になります。私たちはある意味で、飛び立つ飛行機を建設しています。

### 3.3 実証的な観察

前のセクションでは、大規模モデルに関連するいくつかの動機づけと課題を説明しました。このセクションでは、これらの問題が次の 3 つの相互関連した実証的な観察をどのように説明するかを評価します：(1) 大規模言語モデルは急速に広がっています (2) インダストリーはアカデミアに比べてリソース集約型モデルの開発の大部分を担当しています (3) 大規模モデルの展開は既に害と論争を引き起こしています。

大規模言語モデルは急速に広がっています。図 6 は、GPT-3 スケール（100B - 530B）の密集言語モデルの公開開示のタイムラインを示しています。GPT-3 が発表されてから約 1 年後、類似のモデルの発表が続きました。これらのモデルは、世界中の大規模および小規模の民間組織によって開発されました：Jurassic-1-Jumbo [52]、AI21 Labs、イスラエル；Ernie 3.0 Titan [82]、Baidu、中国；Gopher [63]、DeepMind、アメリカ/イギリス；FLAN [83] & LaMDA [79]、Google、アメリカ；Pan Gu [90]、Huawei、中国；Yuan 1.0 [88]、Inspur、中国；Megatron Turing NLG [73]、Microsoft & NVIDIA、アメリカ；HyperClova [47]、Naver、韓国。これは、このようなモデルを構築する経済的なインセンティブと、それらを発表する名声のインセンティブが非常に強力であることを示唆しています。

インダストリーとアカデミアの間のギャップが広がっています。執筆時点では、無料で一般に公開されている最大の言語モデルは BigScience T0（11B）[69]、Eleuther AI の GPT-J（6B）[81]および GPT-NeoX（20B）[50]であり、これらはインダストリーによって開発されたモデルよりも 1〜2 桁小さいです。アカデミアの研究者は（少なくともいくつかの）大きなモデルに簡単にアクセスできることがありますが、通常は（潜在的に高価な）企業管理の API を通じてのみアクセスできます。これは、高い計算能力の研究がアカデミアからインダストリーに移行するより広範かつ長期間にわたる傾向の一部であり、これは数量化できます（付録 A.7 を参照）。図 7（左）は、近年、大規模 AI 実験に必要な計算が 10 年前と比較して 30 万倍以上増加したことを示しています。このリソースの強化に伴い、アカデミアからの結果の割合が対応する（かつ急激な）下落が見られます（図 7、右）。これは、学術界が科学的好奇心に強く動機付けられており、安全性に対する研究を行う確固たる姿勢があっても、高い財政的およびエンジニアリングコストに対しては重要な課題を抱えている可能性があることを示唆しています。

害と論争。大規模生成モデルの展開によって引き起こされる害の例は多数あります。例えば、AI システム Tay は適切に検証される前に展開され、憎悪的な言葉を生成しました[87]。また、言語モデルがトレーニングデータを記憶し（これには個人情報が含まれる場合もあります）、ディスインフォメーションキャンペーンを支援することが示されています[14, 15, 58]。さらに、このようなモデルを展開する組織に対して批判的な人々は、その懸念を述べたことで直接的な害を被り、時には多くの論争が起こりました[72]。立法者もこれらの問題に積極的に取り組んでいます。例えば、欧州委員会の提案した AI 法案は、「高リスク」な AI システムの展開と監視の方法を標準化しようとしています[15]。これは、責任あるモデルの開発と展開のための基準と規範が非常に必要であり、不足していることを示唆しています。

## 4 良好な展開を促進するための介入

第 2 章で説明した大規模生成モデルの特徴と、第 3 章で議論したモデルの開発と展開のさまざまな動機を考慮すると、潜在的な危険性があるにもかかわらず、大規模生成モデルはますます開発および展開されるでしょう。ここでは、これらのモデルが肯定的な方法で開発および展開される可能性を高めるための技術的および政策的な介入を概説します。各介入については、関連する取り組みに関する文献を参照します。さらに、各介入の具体的な実装方法と注意事項も提供します。

1. インダストリーとアカデミア間の計算の非対称性を減少させる。
   第 3.3 節で示されているように、私企業の組織が大規模生成モデルの主要な開発者および展開者です。これにより、アカデミックおよび政府機関などの他の主体は、これらのモデルの特異的な技術的特徴を理解するために十分な位置になく、それゆえにこれらのモデル固有の問題を研究する能力が低下します。第 3.2 節で概説したように、主な制約はモデルトレーニングのための財政的およびエンジニアリングリソースです。したがって、より大きな科学コミュニティがこれらのモデルを分析するための実験的なインフラストラクチャ 16 を作成し、このインフラストラクチャをサポートおよび効果的に活用するために、アカデミックおよび政府機関はまた、産業界に行くかもしれない技術的な才能を雇用および保持できる必要な財政的および構造的な投資の方法を見つける必要があります。これは、アカデミックおよび公共部門の動機が利益よりもむしろ知識の追求に基づいており、大規模生成モデルの分析および探索において私企業よりも多様な専門知識を提供できる可能性があるため、重要です。大規模モデルはリソースが豊富ですが、他の一部の分野での学術の「ビッグサイエンス」プロジェクトよりもはるかに費用がかかりません。例えば、大型ハドロン衝突型加速器は 50 億ドルの建設費がかかりました [48]。国際熱核融合実験炉の見積もり建設費は 100-150 億ドル [18]、平方キロメートルアレイの見積もり費用は約 10 億ドル [16]、長基線中性子施設および深地下中性子実験の見積もり費用は 24 億ドルとされています [78]。これに比べて、GPT-3 などの先進的な生成モデルのトレーニングは 100 万から 1000 万ドルの範囲でコストがかかるため、現在のフロンティアよりもはるかに大きなモデルを開発するためのインフラストラクチャには先例があります。

実装経路：国は、アカデミック研究者に対して大幅に補助されたおよび/または無料の計算リソースにアクセスすることを容易にする「国立研究クラウド」を開発および展開することを検討する可能性があります。既存の例として、Compute Canada があります [19]。また、アメリカ政府の National AI Research Resource task force が分析しているインフラストラクチャや、フランス政府が部分的に補助しているスーパーコンピュータを活用した「ビッグサイエンス」プロジェクトなどの将来のイニシアティブも検討されています [20]。Stanford からの最近の研究も、この実装経路について詳しく探求しています [41]。

2. モデルの「レッドチーミング」方法の向上。これらのモデルに関する一部の課題は、そのオープンエンドの性質から生じています（なめらかなおよび急激な能力スケーリングがこれを複雑にしているかもしれません）。そのため、展開前に害を発見するために、モデルの入力および出力空間をより効果的に探索する方法を開発する必要があります。これは、コンピュータセキュリティ業界で一般的な「レッドチーム」アプローチをモデルに適用することができる、コンピュータセキュリティ業界で人気のある「レッドチーム」アプローチをモデルに適用することができる可能性があります。これは AI コンテキストでも適用できます [6, 13]。これは、静的ベンチマーク（例えば、コンピュータビジョンシステムの弱点を探るための敵対的データセット [37]）と、これらのモデルとの様々な層での交流（例えば、会話[4, 89]）を実施する人間による継続的な評価の形を取るべきです。さらに、これらの評価で見つかった内容に応じてモデルを更新する方法も考慮すべきです。[訳註：red team はセキュリティ分野の用語。対義語は blue team であり、red が攻撃側(いわゆるホワイトハッカー)、blue が防衛を担当する。それらが共同でやるセキュリティ対策は purple team などと呼ばれることもある]

実装経路：モデル開発者は、モデルに対する内部のレッドチーミングアプローチに投資し、レッドチーミング時の技術、データセット、ポリシーの選択肢についての発表を行うべきです。これにより、モデルのレッドチーミング方法について共有の意識が高まります。また、「レッドチーミングをサービスとして提供する」という商業市場も開発できる可能性がありますが、この分野のコミュニティ研究が前提となるかもしれません。AI 開発者は、特定の AI システムを壊す方法を再現可能に示す人々に賞金を与える「バグバウンティ」イニシアティブを作成することも検討すべきです [46]。最後に、人手によるレッドチーミングを自動化手法で補完または拡張する方法も考慮する必要があります [58]。

3. 新しいガバナンス構造と政府の介入を探索し、プロトタイプを作成する。モデルの能力とリソースの集中性がさらに拡大する場合、開発と展開に関する私企業のアクターのインセンティブを変えるガバナンス構造を探索することが賢明かもしれません。これには、ソフト規制（産業界、アカデミア、市民社会、政府によるボランティアベストプラクティスの作成）とハード規制（これらのベストプラクティスを標準と立法に変換する）の組み合わせが含まれます。政府はまた、アクターが有益なシステムを開発および展開する可能性を高めるための規制アプローチを探求するべきです。

実装経路：AI 開発組織は、より広範なステークホルダーがモデルの展開決定に影響を与えることができる新しいガバナンスおよび監督構造を実験するべきです。これには、監督機能が含まれる可能性があり、監督機関の勧告から逸脱した場合に組織を公然と批判し、非難することができる機能です。多様なステークホルダーが組織の利益を代表する役員を選出する新しいガバナンス形態など、多様なステークホルダーに組織の権限を与える形態も考慮すべきです（例えば、純粋な利益志向ではなく、市民社会やアカデミアの利益を代表する役員を選出できる民間企業）。AI 開発組織は、AI システムの開発と展開に関するベストプラクティスを共同で開発し、それを幅広いステークホルダーからのフィードバックを得るために、標準形成の目的でサードパーティ組織の創設を通じて可能性を追求すべきです。AI 組織のガバナンスの革新とベストプラクティスに関する取り組みとともに、政府は展開されるシステムの利益を保証するための方法を向上させるべきです。具体的には、政府は展開される AI システムの能力（有害であるか有益であるか）を測定および監視するための取り組みを支援すべきです[86]。また、AI モデルおよび AI 開発プロセスの監査に焦点を当てたエコシステムの創造を支援すべきです[55, 65, 66]。

4. モデル評価のためのツールを改善する。これらのモデルのオープンエンドの性質とスケールを考慮すると、研究者はこれらのモデルを包括的かつ効率的に評価するためのより多くのツールを持つことが有益です。この分野でより多くのオープンソースのツールとフレームワークを作成する方法を見つけることができれば、広範なモデル開発エコシステムに利益をもたらすことができます。特に、非常に広範な評価を行うためのツールや、新しい能力を検索する（プロンプトを横断して検索するなど）評価を行うツールが価値があります。既知の能力を測定する固定評価データセットだけではなく、新しい能力を測定するためのツールが求められます。

実装経路：研究資金提供機関は、評価システム（ソフトウェア、テスト、ベンチマーク）を構築し、これらを批判する研究者に資金を割り当てるべきです（例：[21, 64]を参照）。私企業や独立した研究機関は、大規模生成モデルを理解し評価するためのツールを開発するためにさらに投資すべきです。Eleuther の「Language Model Evaluation Harness」[28]、BIG-bench ベンチマーク[21]、HuggingFace の「BERTology」ツール[22]などの既存の例があります。

5. 能力の急激なジャンプに関する理解を向上させる。第 2.2 節では、能力の急激なジャンプ（急激な能力スケーリング）のいくつかの例を挙げました。経験的に、急激なジャンプは少数のタスクでしか発生せず、同時に特に珍しいわけではありません。それらがどの程度頻繁に発生し、どのタスクの種類に特徴的なのか、なぜそれらが発生するのか、それらが発生する前に予測するための先行指標はあるのか、といった問いに答えることで、大規模モデルの最も驚くべき振る舞いの一部を解明するのに役立つかもしれません。特に将来の AI の安全性問題にとって重要かもしれません。

実装経路：大規模モデルの研究と可能な限り商業タスクでの急激なジャンプの系統的な実証研究は、それらがどの程度一般的であるか、いつ発生するかについての洞察を提供するのに役立つ可能性があります。これを研究するための 1 つのアプローチは、解釈性の研究（例：[18]）を通じて行われる可能性があり、特にメカニスティックな解釈性として知られる新しいアプローチ[26]が含まれます。トランスフォーマ（この論文で議論されている多くの生成モデルの基盤となるもの）が実行する計算を逆にエンジニアリングする試みにより、研究者はモデルの振る舞いをより良く理解する方法を提供されます。

## 5. 結論

本論文では、大規模生成モデルが高い予測可能性と高い予測不可能性という逆説的な組み合わせを持つことを示唆し（および証拠を提供し）、モデルの損失がトレーニングに費やされるリソースに関連して改善し、多くのタスクでの性能向上と緩く相関する傾向がある一方で、特定のモデルの能力、入力、および出力は事前に予測できないことを述べました。前者はこれらのモデルの急速な開発を推進する一方、後者は開発と展開の影響を予測するのが難しくなります。これらの特性が組み合わさり、AI の開発の景色を変え、より多くのアクターがこれらのモデルを構築する可能性が高くなることを説明しました。率直に言うと、ここで述べた現状からすると、次の数年間はますます多くのアクターがますます大きなモデルを構築し、これらのアクターが潜在的に（予測不能な可能性がある）社会的な影響を及ぼす可能性があるにもかかわらず、これらのモデルを展開する強力な動機を持つことが予想されます。さまざまな介入（私たちが論文で提案したものを含む）は、この動向を変えることができますが、それでも私たちは現在の状況から始め、改善を続けなければなりません。
