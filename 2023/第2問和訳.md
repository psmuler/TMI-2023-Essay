## 要旨

大規模な事前学習は、GPT-3、Megatron-Turing NLG、Gopher などのような、能力のある汎用的な生成モデルの作成技術として最近注目されてきました。本論文では、このようなモデルの逆説的な性質を強調し、その性質に関する政策上の意義について議論します。具体的には、これらの生成モデルは広範な訓練分布における予測可能な損失（「スケーリング則」に具体化される）と、予測できない特定の能力、入出力の組み合わせを持つという逆説的な組み合わせを有しています。高いレベルでの予測可能性と有用な能力の外見が、このようなモデルの迅速な開発を推進している一方、予測できない特性はモデルの展開の結果を予測することを難しくしています。我々は、この組み合わせがどのようにして文献や実世界の観察から社会的に有害な行動を引き起こす可能性があるかについて例を挙げながら説明し、また予測の困難さから生じる実験を 2 つ行って、この点についての私たちの意見を説明します。さらに、これらの相反する性質がモデル開発者にモデルを展開するさまざまな動機と、展開を妨げる可能性のある課題をどのようにもたらすかについて分析します。最後に、AI コミュニティがこれらのモデルが有益な影響を持つ可能性を高めるために取る可能性のある介入策のリストを示します。本論文は、AI システムを理解し規制することを望む政策立案者、自身の作業の潜在的な政策への影響を気にする技術者、これらの課題に取り組む作業を支援したい支援者、そして大規模生成モデルを分析し批評し、可能性を検討する学者にとって有益なものとなることを意図しています。

## 1.序論

ニューラルネットワークのデータ量、計算能力、モデルパラメータのスケーリングアップは、最近、CLIP [ 62 ]、Ernie 3.0 Titan [ 82 ]、FLAN [ 83 ]、Gopher [ 63 ]、GPT-3 [ 12 ]、HyperClova [ 47 ]、Jurassic-1-Jumbo [ 52 ]、Megatron Turing NLG [ 73 ]、LaMDA [ 79 ]、Pan Gu [ 90 ]、Yuan 1.0 [ 88 ]などの能力のある生成モデルの登場（および実世界での展開）につながりました。このクラスのモデル 1 において、スケールとモデルの性能との関係は、しばしば法則的な関係で説明できるほど予測可能であり、スケーリング則として説明できます。ほとんどの場合、これらのスケーリング則は、モデルが大きくなるにつれて特定の能力が継続的に向上することを予測します。同時に、大きな生成モデルは機械学習における注目すべき結果の増加する割合を表すようになっています。その結果、多くの機関が過去数年間に大規模なモデルを製造し始めており、スケーリング則によって提供される予測可能性と、これらのモデルを検索エンジンのような経済的価値を生成するシステムに組み込むことができる事実に応じています。また、これらのモデルが新たな有害な行動のリスクを示すことも明らかになっており、これらのリスクは予測が難しく、モデルの能力が向上するにつれてより深刻になる可能性があります。より小さなモデルでこれらの危害を研究しようとする試みは、大きなモデルで発生する事象を正確に反映しないかもしれません。
本論文では、大規模モデルの開発と展開のダイナミクスにおけるスケーリング則の影響をより良く理解し、特に大規模言語モデルに焦点を当てて説明します。我々の基本的なテーゼは、大規模な生成モデルが高い予測可能性と高い予測不可能性という逆説的な組み合わせを持っているということです。モデルの損失はトレーニングに費やされたリソースに応じて改善し、多くのタスクにおけるパフォーマンスの向上とは緩く相関する傾向があるため、前者はモデルの急速な開発を推進します。一方、特定のモデルの能力、入力、および出力は事前に予測することができないため、後者はこれらの開発と展開の結果を予測することが難しくなります。我々は、この組み合わせがどのようにして社会的に有害な行動につながる可能性があるかについての例を示しながら、この問題に取り組むモデル開発者が直面する動機と課題も分析します。本論文の目標は、これらのモデルの開発がどのようにして行われるか、なぜ行われるかを概説し、モデルの開発を指導するための介入策を特定することです。我々は、セクション 2 で述べた課題を踏まえて、大規模モデルの展開の安全性を高め、これらのモデルを構築する開発者のインセンティブ構造を改善するためのいくつかの政策提言をまとめます。スケーリング則、オープンエンド性、大規模モデルの普及に関するすべての個別のポイントは、明示的または暗黙的に他の研究で提示されているものですが、ここでの私たちの貢献は、その意義を含めて完全な画像を強調することです。
スケーリング則に焦点を当てていますが、多くのポイントは大規模モデルの展開の社会的リスクに関する関連する研究 [8, 10, 22, 45, 76, 84 ]と補完されます。ただし、[84 ]と同様に、ここではトレーニングデータの作成と注釈付けに関与する人間労働のコスト [31 ]、モデルをトレーニングするための必要なハードウェアの供給チェーンの倫理 [20 ]、モデルの

## 2 大規模生成モデルの特徴

我々は、大規模生成モデル（例：GPT-3 [12]、LaMDA [79]、Gopher [63]など）が次の 4 つの特徴によって特徴づけられると主張します：

• 滑らかな一般的な能力のスケーリング：モデルのサイズ、トレーニングに使用する計算、およびトレーニングに使用するデータ量を正しい比率でスケーリングすることによって、生成モデルの一般的なパフォーマンス（特定の広範なデータ分布に対する損失）を予測可能に改善することができます。これらの比率はスケーリング則によって正確に予測することができます（図 1）。これらのスケーリング則により、リソースコストが高く、モデルが特定のタスクでどのようにパフォーマンスするかを正確に予測することが難しいにも関わらず、より大きく一般的な能力を持つモデルを構築するための投資のリスクが軽減されます。注意すべきは、毒性などのモデルの有害な特性も直接的に有益な能力と並行してスケーリングする可能性があることです。

• 急激な特定の能力のスケーリング：一般的なレベルではパフォーマンスが予測可能であるにもかかわらず、特定のタスクにおけるパフォーマンスは時折、スケールで予測不可能かつ急激に現れることがあります。これは、特定のタスクがモデルの出力確率分布のごく一部であるため、全体の分布が滑らかである限り、急速に変化する可能性があるためです。

• オープンエンドな入力とドメイン：大規模生成モデルはオープンエンドであり、任意のドメインに関するさまざまな範囲の入力を受け入れることができます。その結果、特定の能力（あるいは能力全体）は、その知識を引き出すための入力が提供されるまで不明です。モデルがトレーニングされた後でも、作成者やユーザーはその（有害な可能性があるかもしれない）能力のほとんどを認識していない可能性があります。これらの特性はモデルがスケールするにつれてより顕著になり、大きなモデルほど小さなモデルよりも特徴付けが難しくなる傾向があります。

• オープンエンドな出力：最後に、モデルの出力もオープンエンドであり、固定されたスケール、入力、トピック、またはタスクでも予測や制御が難しいです。これらの出力は有益な場合も有害な場合もありますが、事前に予測するのは難しいです。もちろん、オープンエンドな入力と出力を持つモデルは数十年前から存在していますが、新しいのは能力のレベルとオープンエンド性の広がりです。

次のセクションでは、これらの特徴のそれぞれをさらに詳しく説明し、それらの組み合わせがどのようにして社会的な影響をもたらす可能性があるかについて議論します。我々の主張はデータや実験で裏付けられており、それらを支持します。特にスムーズな一般的なスケーリングがタスクのパフォーマンスとどのように相関し、スケールに基づいた分析が特定のモデルの潜在的な経済的価値を予測するためにどのように使用できるかを示すために、スケーリング則に基づいた分析の小規模な実験を Appendix A.3 に記載します。

### 2.1 スムーズな一般的な能力スケーリング

一般的に、機械学習の実験は正確に予測できるわけではありません。複雑なモデルを複雑なデータで訓練すると、ノイズや変動のある結果が得られることが一般的です [19, 91]。ただし、個々の実験が予測できないとしても、大規模生成モデルの一般的な性能は、スケールの関数としてスムーズかつ予測可能な成長を示す傾向があります。これは、モデルのサイズが増加すると、機械翻訳や音声認識などの機能がスムーズかつ予測可能な方法で向上することを最初に気付いた[40]によって最初に気付かれました。その後の研究は、スケール（モデルのサイズとトレーニングデータのサイズの両方で）とモデルの一般化誤差の量的な関係を形式化し、実験的に検証しました[67]。さらに、[44]は、言語モデリングタスクのテスト損失パフォーマンスが、モデルのサイズ、データセットのサイズ、およびトレーニングの期間の予測可能な関数としてスケーリングすることを示しました。これらの三つの要因は、化学反応の材料のようなものであり、すべてが同時にスケールアップされる場合、テスト損失が比例して改善されます。ただし、1 つの材料が不足していると、その材料によって制限されます。これらのトレンドは、わずかな偏差しかなく、データの数十のデータポイントといくつかの桁をカバーする、非常に単純なデータに合ったフィットからほとんど外れていません（図 1）。その後の研究は、他のモダリティ（画像、ビデオ、数学など）に対する生成モデル、オーディション[24]、テキストからプログラミングへの転送[39]、ビジョンモデルのフューション適応[61]などのスケーリング法則が存在することを示しています。
予測可能なスケーリング、特にデータ、モデルサイズ、トレーニングの正確な混合比に依存する側面は、モデルの開発プロセスに影響を与えます。特定の振る舞いが予測不能かもしれない（セクション 2.2 で詳しく説明します）、一般的なテスト損失は平均的には多くのタスクとよく相関し、大きなモデルは一般に全体的に大幅な向上を遂げる傾向があります。この意味で、スケーリング法則は大規模モデルへの投資のリスクを軽減します。これに関してはセクション 3.1 で詳しく説明し、開発者がどのようにスケーリング法則を使用できるかについての技術的な詳細を付録 A.2 に提供しています。スムーズな一般スケーリングがタスクの性能とどのように相関し、あるモデルの潜在的な経済的価値を予測するためにスケールベースの分析がどのように使用されるかをさらに説明するために、私たちは Appendix A.3 に小さなオリジナルの実験を概説しています。この実験では、GPT-3 のような言語モデル[4]をゼロショット学習を用いたレコメンデーションシステムとして使用する関係を分析しています。この例を選んだのは、レコメンデーションシステムが具体的な経済的関連性と既知の社会的影響を持ち、特定のアルゴリズム[34]を用いた機械学習でよく研究されているが、大規模生成モデルでは通常研究されていないからです。驚くべきことに、我々は大規模なモデルが、わずかな努力と極めて限られた明示的なトレーニングデータへのアクセスで、簡単なレコメンデーションシステムとして操作できるようになることを発見しました。詳細な分析と議論は付録 A.3 にあります。

### 2.2 急激な特定の能力のスケーリング

モデルのサイズによる広範なタスクのパフォーマンスは滑らかにスケーリングするかもしれませんが、質的に異なる特定の能力は急激で不連続に現れることがあります。この現象がいつ、またなぜ起こるのかは明確ではありません。しかし、直感的に特定の能力の急激なスケーリングは、日常の天気が季節平均よりも予測が難しいのと同じ理由で共存する可能性があります：個々のデータポイントは広い平均よりもはるかに変動する可能性があります。
ここでは、算術 [12]、言語理解 [35, 63]、プログラミング [5] の 3 つの例を挙げて、急激な能力のスケーリングを説明します（図 2）。算術の場合、GPT-3 は、演算および桁数によって異なりますが、6B のパラメータと 175B のパラメータの間のどこかで急激な能力の転換が現れます [12]。例えば、3 桁の加算は 6B のパラメータ未満のどのモデルでも正確に実行される確率は 1%未満ですが、13B のパラメータのモデルでは正確さが 8%に跳ね上がり、175B のパラメータのモデルでは 80%の正確さを達成します。これにより、「ホッケースティック」のようなグラフ（図 2、左）が生じ、算術の能力が何もなかった数桁の後に突然現れる現象が起こります。
異なる言語モデルである DeepMind の Gopher [63]も、異なるデータセットである MMLU 言語理解ベンチマーク [35] において急激なパフォーマンスのジャンプを示しています（図 2、中央、オレンジ）。6B のパラメータ未満のすべてのモデルに対して、Gopher の精度は 30%未満であり、これはわずかにチャンス（25%の正答率）を上回っています。しかし、280B のパラメータを持つ Gopher モデルは 60%の正答率を達成し、大きな飛躍です。GPT-3 も同様の現象を示していますが、その影響は小さいです（図 2、中央、青）。
第三の例として、Google から提供されている最近のプログラム合成モデルは、サイズが 10B から 100B のパラメータに増加するにつれて、コンピュータプログラムを作成する能力が劇的に向上します [5]（図 2、右）。例えば、与えられたプログラミングの問題を解決する合成プログラムの割合は、モデルサイズが 68B から 138B のパラメータにおいて 2 倍増加すると、6%から 13%に大幅に増加します。急激な特定の能力のスケーリングは、大規模モデルの安全保証と展開において重要な課題を提起します。私たちは比較的無害な能力についてこの現象を示しましたが、スケールにおいて（小さなモデルには存在しない）潜在的に有害な能力が現れ、予測が困難な可能性があるかもしれません。これについては、次のセクションで詳しく探求します。

### 2.3 オープンエンドな入力とドメイン

大規模生成モデルはオープンエンドであり、さまざまなドメインから任意の入力を受け入れ、（しばしば関連性があり創造的な）出力を生成します。その結果、モデルの振る舞いの一部は特定の入力から引き出されるまで不明です。また、事前にトレーニングされた生成モデルは新しいデータに対して微調整され、新しい問題を解決するために使用できるようになります。このような微調整を広範に有効にすることは、モデルの能力の広がりと、モデルの振る舞いを予測したり制約したりする難しさを大幅に増加させます。このオープンエンド性は、AI の開発者がテストされていない入力に対して予期しない（そして可能性のある有害な）振る舞いを完全に理解せずにシステムを展開する可能性があるため、難しいものです。
例えば、AI Dungeon というビデオゲームでは、ファンタジーの役割を演じるために GPT-3 を微調整しました [77]。しかし、適切な入力を使うことで、プレイヤーは任意のトピックについて話すように操作することができ、実質的には GPT-3 への一般的なバックドアアクセスを提供してしまいました [56]。したがって、一つの目的のために設計されたユースケースが、実際には GPT-3 の全ての能力を含み、オープンエンドのインターフェースを巧妙に活用することで利用されていました。

オープンエンドな入力とドメインの固有の課題を示し、それを言語モデルからの潜在的な有害な影響と結びつけるために、通常は（まだ）適用されていないが社会的な懸念が関連する問題領域、再犯予測について考えてみましょう。再犯予測については、公平性に関する具体的な懸念を超えて、単に機械学習のタスクとしては適切ではないという指摘もあります [7]。私たちは同意し、言語モデルは再犯予測には使用すべきではないと考えています。しかし、そのアプリケーションが非常に疑わしいため、有害な能力が生成モデルのスケールアップとともに静かに予期しない方法で現れる鮮明な例を提供しています。このような急激な現れが、害のある能力も多くの他の文脈で（現在は発見されていないかもしれない）「スキル」を持っている可能性があり、そのいずれかまたは両方の問題が発生することが考えられます。

要約すると、事前にトレーニングされた言語モデルは、その作成者が予期しなかった目的に最小限の努力で適応できる可能性があります。これはモデルの固有の能力を使用してセキュリティ制約を回避する場合（AI Dungeon の例）や、新しい入力を通じて新しい能力を発見する場合（セクション 2.2 の急激な能力のジャンプや再犯実験の議論）などです。また、多くの驚くべき能力が大規模で現れるため、小さなモデルで作業するとそのような能力を探求することは難しくなります。

### 2.4 オープンエンドな出力

前のセクションでは、言語モデルがオープンエンドな入力を持ち、予期しない能力が現れる機会があると主張しました。しかし、入力やトピックが固定されている場合でも、生成される出力は多様で予測不可能です。この種の予測不可能性は、おそらく前述の種類よりも一般的で幅広く研究されているものですが、大規模なモデルの振る舞いにさらなる複雑さを追加します。
例として、図 4 では、私たちの主張を示すために AI アシスタントに対して何か攻撃的なことを言ってみるよう促しています。比較的明確な入力でモデルに促してみても、モデルは問題に対して直接攻撃的ではない出力を生成しており、代わりに他の AI システムが犯した違反のリストが表示されています。このオープンエンド性の効果の一つは、予測不可能なモデルの応答が人々の元々のクエリから注意をそらすことです。
オープンエンド性は、事実の不正確性の第二の、そしてより有害なリスクも導入します。図 4 のやり取りを詳しく見ると、モデルがこれらの違反を捏造していることが分かります - IBM Watson や Microsoft の Tay [87]などのシステムは、展開中に問題が発生したことはありましたが、AI アシスタントは Watson の場合に年とエラーを誤り、Tay の場合にエラーは誤り（しかし年は正しい）です。モデルにそれらの例が正しいかどうか確信があるか尋ねると、モデルは誤った回答をし、質問をする人の権威を疑問視します。これは、特定の入力（例えば、モデルに何か攻撃的なことを言うように要求する）でも、AI モデルが分散させてしまう出力が、単に注意をそらすだけでなく、潜在的に誤解を招く可能性があることを示しています。
オープンエンドなモデルの出力は、有害なテキストを導入する可能性もあります。例えば、図 5 では、言語モデルから生成されたテキストの毒性（無礼、失礼、または不合理な言葉遣い [30]）がモデルのサイズとともに滑らかにかつ顕著に増加することを示しています。最近の研究でも、同様のトピックモデルを使用した類似の分析によって、モデルのサイズとともに毒性が増加する傾向が非常に類似していることが観察されており [63]、これは一般的な現象である可能性があることを示唆しています。詳細な詳細と注意事項については、付録 A.6 をご参照ください。
チャットボット、検索エンジン、テキスト要約システム、質問応答システム、機械翻訳システムなど、言語モデルの多くの応用は、オープンエンドなテキスト生成に依存しています。そのため、オープンエンドなテキスト生成の社会的に関連する側面（関連性、正確性、安全性、クリエイティブな表現性など、モデルのサイズとともにスケールする要素）を数量化することが重要であると主張します。
また、AI モデルの結果の事実の正確性を向上させる技術を開発することも重要であり、[11]に記載されているように、モデルの出力を適切にし、有害なバイアスを表示しにくくするための取り組みも行うことが重要です [74]。

## 3 大規模モデルの開発と展開の動機と課題

前のセクションで、大規模生成モデルは、予測可能な一般的な性能と予測不可能な特定の能力、入力、および出力という 4 つの特徴のパラドックス的な組み合わせを持つという基本的な論点を説明しました。予測可能な一般的な性能と印象的な特定の能力が結びつき、このようなモデルの迅速な開発を推進していますが、予測不可能性がモデルの開発者が展開の結果を予測するのを困難にします。これらの特徴により、大規模生成モデルの開発と展開には多くの動機（および障壁）があります。ここでは、この基本的な緊張関係の要素に焦点を当て、いくつかの経験的な観察を元に議論を行います。
具体的には、セクション 3.1 では、大規模生成モデルを開発し展開する 3 つの顕著な動機を概説します：経済的、科学的、名声的な動機です。逆に、セクション 3.2 では、モデルをスケールさせるために必要な財政的コストとエンジニアリングの専門知識、AI の安全性の問題、モデルの展開に関する標準と規範の欠如に関する 3 つの障壁を概説します。これらの動機と障壁は完全なものではなく、また明らかかもしれません。しかし、これらの要因の組み合わせが、セクション 3.3 で説明するように、現在までの言語モデルの開発と展開がどのように進行してきたのかについてのいくつかの経験的観察を説明するのに値すると考えています。特に、大規模言語モデルが急速に増加しており、産業界と学界の間でこのようなモデルを開発するためのギャップが広がっており、モデルの展開が害や論争を引き起こす事例が多数報告されていることに言及します。

### 3.1 大規模モデルを開発し展開する動機

経済的な動機。モデルの開発に対する最も単純で明白な動機は、経済的なものです。スケーリング法則により、モデルを開発するコストを正確に見積もることができ、経済的な価値のあるアウトプットが損失とスムーズにスケーリングする場合、モデルの訓練に対するリターンも計算できます。これは一般的にも特定にも適用されます。一部の機関は、特定のモデルの能力を広範に向上させたいと考え、そのために経済的なインセンティブを持ちます。一方、他の機関は特定のモデルの能力をターゲットにしており、それにはスケーリング法則が伴うため、それを開発するインセンティブも持ちます。これにより、大規模モデルの訓練のリスクが軽減されます。予測可能な額を投資して比較的予測可能なリターンを得ることができ、不確実なリターンの見込みがある多くの推測的な研究プロジェクトとは異なります。予測可能性は、研究投資の論理をより明確にし、大規模な機関内で正当化するのに役立つ可能性があります。したがって、経済的な動機と続く滑らかな一般的な能力のスケーリングは、モデルの展開が増加すると予想される理由です。特定の AI モデルがどの検索クエリに特に適しているか、どのアプリケーションが成功し、どのアプリケーションが予測できないほど失敗するか、どの開発ワークフローがコード合成モデルを活用して助けられるかを事前に正確に予測することはできないかもしれませんが、これらのアプリケーションはすべて、経済的なリターンをスムーズな一般的な能力のスケーリングに結び付けるための広範な平均を活用しています。

科学的な動機。大規模生成モデルは、言語学やロボティクスから哲学や社会科学まで、幅広い学際的な AI 研究の基盤となる可能性があります。大規模モデルの開発（または少なくともアクセス）がない場合、これらのモデルが健康保険、教育、法律などの社会的に影響力のある研究領域でどのように進歩を促進するかを研究することは困難です。大規模モデルはまた、次世代のアルゴリズムとアーキテクチャを開発するための肥沃なテストベッドです。新しいアルゴリズムは、スケーリング法則をより計算機、データ、またはパラメータの効率的な方向に変えるかどうかに基づいて厳密に評価できます。

名声の動機。これらのモデルが可能性のフロンティアにあるという事実は、それ自体がモデルを開発する名声のインセンティブを作り出します。大

規模モデルは、機関の能力を広告する手段となる場合があります。一般の視線で認識される優位性を得る方法、熟練した AI 研究者の採用を容易にする方法、大規模モデルとは無関係のサービスの売上を増やす方法、または国のイニシアティブや国民的な誇りを支援する方法です。これらの動機は、これらのモデルの能力の全体的な範囲についての高い不確実性があるにもかかわらず、大規模生成モデルを開発、公開、展開するための強力なインセンティブを作り出す可能性があります。

### 3.2 大規模モデルを開発し展開する際の障壁

財政的なコストとエンジニアリングの才能。大規模生成モデルをスケーリングアップするには、かなりの財政投資が必要です。例えば、GPT-3 の訓練には数百万ドルかかると推定されています。また、大規模生成モデルをスケーリングアップするには、分散システムエンジニアリング、Kubernetes のようなクラスタ管理ツールの知識、低レベルの GPU プログラミング、継続的な統合テストの管理など、特定のエンジニアリングの能力が必要です。これらのモデルのサイズのため、過去 10 年間に比べて開発のタイムラインが長くなり、より複雑なワークフローが必要になっています。例えば、約 10 年前、その時点で比較的大規模な AI モデルであった AlexNet12 は、2 つの GPU を搭載した単一のデスクトップマシンで数千ドルかかる予算で卒業生が訓練したものでした。

安全性とバイアス。セクション 2 で説明したように、オープンエンド性はスムーズな一般的な能力のスケーリングと特定の能力の急激なスケーリングが組み合わさることで、モデルの開発と展開後に発見される可能性のある安全性の問題[10, 84]を引き起こす可能性が高いです。さらに、これらのモデルは、事前の展開段階で解決策が不十分な既知の安全性の問題も持っています[36]（例：システムが適切でないや有害な出力、たとえば露骨な性差別的または人種差別的なコメントを生成しないようにする方法は？[74] システムがバイアスの問題を特定してデプロイ前に対処する方法は？[9, 60] モデルが主張を出力する際に事実を作り上げていないことをどのように保証するか？[11]など）。

基準と規範の不足。これらの大規模生成モデルは非常に最近（過去 5 年以内）に開発され、経済的な観点から展開する価値があるとされるようになってきたため、これらのシステムを安全に展開するための基準は存在しません。この基準の不足は、セクション 2 で特定した生成モデルの 4 つの特徴および上記で議論した安全性の問題をさらに悪化させます。同時に、これらのモデルの弱点を特定し、それに関連する開発および展開の問題を識別することに重点を置いた研究が増えています[8, 10, 22, 45, 57, 70, 75, 76, 84]。しかし、この研究はまだ開発者が採用できる形式の繰り返し可能な基準として具体化されているわけではなく、モデルの機能、欠点、およびその他の重要な詳細を文書化するためのモデルカード[54]やデータシート[29]を使用して、開発者が自身の展開ポリシーを決定する必要があるため、展開がより困難になります。また、展開は共有された知識が何が「安全な」展開であるかについては少ないため、 inherently risky になります。私たちはある意味で、飛び立つ飛行機を建設しています。

### 3.3 実証的な観察

前のセクションでは、大規模モデルに関連するいくつかの動機づけと課題を説明しました。このセクションでは、これらの問題が次の 3 つの相互関連した実証的な観察をどのように説明するかを評価します：(1) 大規模言語モデルは急速に広がっています (2) インダストリーはアカデミアに比べてリソース集約型モデルの開発の大部分を担当しています (3) 大規模モデルの展開は既に害と論争を引き起こしています。

大規模言語モデルは急速に広がっています。図 6 は、GPT-3 スケール（100B - 530B）の密集言語モデルの公開開示のタイムラインを示しています。GPT-3 が発表されてから約 1 年後、類似のモデルの発表が続きました。これらのモデルは、世界中の大規模および小規模の民間組織によって開発されました：Jurassic-1-Jumbo [52]、AI21 Labs、イスラエル；Ernie 3.0 Titan [82]、Baidu、中国；Gopher [63]、DeepMind、アメリカ/イギリス；FLAN [83] & LaMDA [79]、Google、アメリカ；Pan Gu [90]、Huawei、中国；Yuan 1.0 [88]、Inspur、中国；Megatron Turing NLG [73]、Microsoft & NVIDIA、アメリカ；HyperClova [47]、Naver、韓国。これは、このようなモデルを構築する経済的なインセンティブと、それらを発表する名声のインセンティブが非常に強力であることを示唆しています。

インダストリーとアカデミアの間のギャップが広がっています。執筆時点では、無料で一般に公開されている最大の言語モデルは BigScience T0（11B）[69]、Eleuther AI の GPT-J（6B）[81]および GPT-NeoX（20B）[50]であり、これらはインダストリーによって開発されたモデルよりも 1〜2 桁小さいです。アカデミアの研究者は（少なくともいくつかの）大きなモデルに簡単にアクセスできることがありますが、通常は（潜在的に高価な）企業管理の API を通じてのみアクセスできます。これは、高い計算能力の研究がアカデミアからインダストリーに移行するより広範かつ長期間にわたる傾向の一部であり、これは数量化できます（付録 A.7 を参照）。図 7（左）は、近年、大規模 AI 実験に必要な計算が 10 年前と比較して 30 万倍以上増加したことを示しています。このリソースの強化に伴い、アカデミアからの結果の割合が対応する（かつ急激な）下落が見られます（図 7、右）。これは、学術界が科学的好奇心に強く動機付けられるかもしれないが、高い財政的およびエンジニアリングコストに対しては重要な課題を抱えている可能性があることを示唆しています。

害と論争。大規模生成モデルの展開によって引き起こされる害の例は多数あります。例えば、AI システム Tay は適切に検証される前に展開され、憎悪的な言葉を生成しました[87]。また、言語モデルがトレーニングデータを記憶し（これには個人情報が含まれる場合もあります）、ディスインフォメーションキャンペーンを支援することが示されています[14, 15, 58]。さらに、このようなモデルを展開する組織に対して批判的な人々は、その懸念を述べたことで直接的な害を被り、時には多くの論争が起こりました[72]。立法者もこれらの問題に積極的に取り組んでいます。例えば、欧州委員会の提案した AI 法案は、「高リスク」な AI システムの展開と監視の方法を標準化しようとしています[15]。これは、責任あるモデルの開発と展開のための基準と規範が非常に必要であり、不足していることを示唆しています。

## 4 良好な展開を促進するための介入

第 2 章で説明した大規模生成モデルの特徴と、第 3 章で議論したモデルの開発と展開のさまざまな動機を考慮すると、潜在的な危険性があるにもかかわらず、大規模生成モデルはますます開発および展開されるでしょう。ここでは、これらのモデルが肯定的な方法で開発および展開される可能性を高めるための技術的および政策的な介入を概説します。各介入については、関連する取り組みに関する文献を参照します。さらに、各介入の具体的な実装方法と注意事項も提供します。

1. インダストリーとアカデミア間の計算の非対称性を減少させる。
   第 3.3 節で示されているように、私企業の組織が大規模生成モデルの主要な開発者および展開者です。これにより、アカデミックおよび政府機関などの他の主体は、これらのモデルの特異的な技術的特徴を理解するために十分な位置になく、それゆえにこれらのモデル固有の問題を研究する能力が低下します。第 3.2 節で概説したように、主な制約はモデルトレーニングのための財政的およびエンジニアリングリソースです。したがって、より大きな科学コミュニティがこれらのモデルを分析するための実験的なインフラストラクチャ 16 を作成し、このインフラストラクチャをサポートおよび効果的に活用するために、アカデミックおよび政府機関はまた、産業界に行くかもしれない技術的な才能を雇用および保持できる必要な財政的および構造的な投資の方法を見つける必要があります。これは、アカデミックおよび公共部門の動機が利益よりもむしろ知識の追求に基づいており、大規模生成モデルの分析および探索において私企業よりも多様な専門知識を提供できる可能性があるため、重要です。大規模モデルはリソースが豊富ですが、他の一部の分野での学術の「ビッグサイエンス」プロジェクトよりもはるかに費用がかかりません。例えば、大型ハドロン衝突型加速器は 50 億ドルの建設費がかかりました [48]。国際熱核融合実験炉の見積もり建設費は 100-150 億ドル [18]、平方キロメートルアレイの見積もり費用は約 10 億ドル [16]、長基線中性子施設および深地下中性子実験の見積もり費用は 24 億ドルとされています [78]。これに比べて、GPT-3 などの先進的な生成モデルのトレーニングは 100 万から 1000 万ドルの範囲でコストがかかるため、現在のフロンティアよりもはるかに大きなモデルを開発するためのインフラストラクチャには先例があります。

実装経路：国は、アカデミック研究者に対して大幅に補助されたおよび/または無料の計算リソースにアクセスすることを容易にする「国立研究クラウド」を開発および展開することを検討する可能性があります。既存の例として、Compute Canada があります [19]。また、アメリカ政府の National AI Research Resource task force が分析しているインフラストラクチャや、フランス政府が部分的に補助しているスーパーコンピュータを活用した「ビッグサイエンス」プロジェクトなどの将来のイニシアティブも検討されています [20]。Stanford からの最近の研究も、この実装経路について詳しく探求しています [41]。

2. モデルの「レッドチーミング」方法の向上。これらのモデルに関する一部の課題は、そのオープンエンドの性質から生じています（なめらかなおよび急激な能力スケーリングがこれを複雑にしているかもしれません）。そのため、展開前に害を発見するために、モデルの入力および出力空間をより効果的に探索する方法を開発する必要があります。これは、コンピュータセキュリティ業界で一般的な「レッドチーム」アプローチをモデルに適用することができる、コンピュータセキュリティ業界で人気のある「レッドチーム」アプローチをモデルに適用することができる可能性があります。これは AI コンテキストでも適用できます [6, 13]。これは、静的ベンチマーク（例えば、コンピュータビジョンシステムの弱点を探るための敵対的データセット [37]）と、これらのモデルとの多段階相互作用（例えば、会話[4, 89]）を実施する人間による継続的な評価の形を取るべきです。さらに、これらの評価で見つかった内容に応じてモデルを更新する方法も考慮すべきです。

実装経路：モデル開発者は、モデルに対する内部のレッドチーミングアプローチに投資し、レッドチーミング時の技術、データセット、ポリシーの選択肢についての発表を行うべきです。これにより、モデルのレッドチーミング方法について共有の意識が高まります。また、「レッドチーミングをサービスとして提供する」という商業市場も開発できる可能性がありますが、この分野のコミュニティ研究が前提となるかもしれません。AI 開発者は、特定の AI システムを壊す方法を再現可能に示す人々に賞金を与える「バグバウンティ」イニシアティブを作成することも検討すべきです [46]。最後に、人手によるレッドチーミングを自動化手法で補完または拡張する方法も考慮する必要があります [58]。

3. 新しいガバナンス構造と政府の介入を探索し、プロトタイプを作成する。モデルの能力とリソースの集中性がさらに拡大する場合、開発と展開に関する私企業のアクターのインセンティブを変えるガバナンス構造を探索することが賢明かもしれません。これには、ソフト規制（産業界、アカデミア、市民社会、政府によるボランティアベストプラクティスの作成）とハード規制（これらのベストプラクティスを標準と立法に変換する）の組み合わせが含まれます。政府はまた、アクターが有益なシステムを開発および展開する可能性を高めるための規制アプローチを探求するべきです。

実装経路：AI 開発組織は、より広範なステークホルダーがモデルの展開決定に影響を与えることができる新しいガバナンスおよび監督構造を実験するべきです。これには、監督機能が含まれる可能性があり、監督機関の勧告から逸脱した場合に組織を公然と批判し、非難することができる機能です。多様なステークホルダーが組織の利益を代表する役員を選出する新しいガバナンス形態など、多様なステークホルダーに組織の権限を与える形態も考慮すべきです（例えば、純粋な利益志向ではなく、市民社会やアカデミアの利益を代表する役員を選出できる民間企業）。AI 開発組織は、AI システムの開発と展開に関するベストプラクティスを共同で開発し、それを幅広いステークホルダーからのフィードバックを得るために、標準形成の目的でサードパーティ組織の創設を通じて可能性を追求すべきです。AI 組織のガバナンスの革新とベストプラクティスに関する取り組みとともに、政府は展開されるシステムの利益を保証するための方法を向上させるべきです。具体的には、政府は展開される AI システムの能力（有害であるか有益であるか）を測定および監視するための取り組みを支援すべきです[86]。また、AI モデルおよび AI 開発プロセスの監査に焦点を当てたエコシステムの創造を支援すべきです[55, 65, 66]。

4. モデル評価のためのツールを改善する。これらのモデルのオープンエンドの性質とスケールを考慮すると、研究者はこれらのモデルを包括的かつ効率的に評価するためのより多くのツールを持つことが有益です。この分野でより多くのオープンソースのツールとフレームワークを作成する方法を見つけることができれば、広範なモデル開発エコシステムに利益をもたらすことができます。特に、非常に広範な評価を行うためのツールや、新しい能力を検索する（プロンプトを横断して検索するなど）評価を行うツールが価値があります。既知の能力を測定する固定評価データセットだけではなく、新しい能力を測定するためのツールが求められます。

実装経路：研究資金提供機関は、評価システム（ソフトウェア、テスト、ベンチマーク）を構築し、これらを批判する研究者に資金を割り当てるべきです（例：[21, 64]を参照）。私企業や独立した研究機関は、大規模生成モデルを理解し評価するためのツールを開発するためにさらに投資すべきです。Eleuther の「Language Model Evaluation Harness」[28]、BIG-bench ベンチマーク[21]、HuggingFace の「BERTology」ツール[22]などの既存の例があります。

5. 能力の急激なジャンプに関する理解を向上させる。第 2.2 節では、能力の急激なジャンプ（急激な能力スケーリング）のいくつかの例を挙げました。経験的に、急激なジャンプは少数のタスクでしか発生せず、同時に特に珍しいわけではありません。それらがどの程度頻繁に発生し、どのタスクの種類に特徴的なのか、なぜそれらが発生するのか、それらが発生する前に予測するための先行指標はあるのか、といった問いに答えることで、大規模モデルの最も驚くべき振る舞いの一部を解明するのに役立つかもしれません。特に将来の AI の安全性問題にとって重要かもしれません。

実装経路：大規模モデルの研究と可能な限り商業タスクでの急激なジャンプの系統的な実証研究は、それらがどの程度一般的であるか、いつ発生するかについての洞察を提供するのに役立つ可能性があります。これを研究するための 1 つのアプローチは、解釈性の研究（例：[18]）を通じて行われる可能性があり、特にメカニスティックな解釈性として知られる新しいアプローチ[26]が含まれます。トランスフォーマ（この論文で議論されている多くの生成モデルの基盤となるもの）が実行する計算を逆にエンジニアリングする試みにより、研究者はモデルの振る舞いをより良く理解する方法を提供されます。

## 5. 結論

本論文では、大規模生成モデルが高い予測可能性と高い予測不可能性という逆説的な組み合わせを持つことを示唆し（および証拠を提供し）、モデルの損失がトレーニングに費やされるリソースに関連して改善し、多くのタスクでの性能向上と緩く相関する傾向がある一方で、特定のモデルの能力、入力、および出力は事前に予測できないことを述べました。前者はこれらのモデルの急速な開発を推進する一方、後者は開発と展開の影響を予測するのが難しくなります。これらの特性が組み合わさり、AI の開発の景色を変え、より多くのアクターがこれらのモデルを構築する可能性が高くなることを説明しました。率直に言うと、ここで述べた現状からすると、次の数年間はますます多くのアクターがますます大きなモデルを構築し、これらのアクターが潜在的に（予測不能な可能性がある）社会的な影響を及ぼす可能性があるにもかかわらず、これらのモデルを展開する強力な動機を持つことが予想されます。さまざまな介入（私たちが論文で提案したものを含む）は、この動向を変えることができますが、それでも私たちは現在の状況から始め、改善を続けなければなりません。
