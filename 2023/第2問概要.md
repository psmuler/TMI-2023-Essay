大規模言語モデルにおける逆説的な(paradoxical)性質：

- 全般的な性能の予測可能な損失の減少（スケーリング則）
- 予測できない特定の能力、入出力の組み合わせ

1. ToC

- section 2 - 4 つの主張
  - スケーリング則と下流タスクの性能向上との相関、
  - 特定の能力の急激な成長、
  - オープンエンドの入力の問題点
  - オープンエンドの出力の問題点
- section 3 - 大規模生成モデルの増加が予測される理由
  - 3 つの動機：経済、科学、名声
  - 3 つの障壁：モデル拡大の財務・能力コスト、AI 安全性の課題、モデル配備の規格と規範の欠如
  - モデル開発の急速な進展、産業と学術の隔絶
- section 4 - 政策介入

2-1. スケーリング則と下流タスクの性能向上との相関
複数の事例

> 汎化誤差：未知データに対する予測誤差

![Alt text](<第2問図表/Screenshot 2023-08-22 at 21.51.22.png>)

- 機械翻訳や音声認識の機能がモデルのサイズ増加に対してスムーズに増加[40],とその一般化[67]
- 言語モデリングタスクのテスト損失パフォーマンスは、モデルのサイズ、データセットのサイズ、およびトレーニングの期間の関数としてスケーリング[44]
- その他のモダリティに対する生成モデル[24]
- text2code[39]
- few-shot の画像生成モデル[61]

影響は 3.1 で議論

推薦システムへの利用の可能性[付録 A.3]

2.2 特定の能力の急激な成長、

個別の能力は、全体平均と異なり急激な変化を示すことは十分に考えられる。
算術 [12]、言語理解 [35, 63]、プログラミング [5]で急激な能力の増加が見られた。

![Alt text](<第2問図表/Screenshot 2023-08-22 at 21.51.40.png>)

2.3 オープンエンドの入力の問題点

予測不能性は、有害な能力についても言える。潜在的に有害な能力が現れ、予測が困難な可能性があるかもしれない。
生成 AI は fine-tuning して使われることが多いが、振る舞いは完全に理解できない。

> AI Dungeon の事例：GPT-3 の fine-tuning を行ったが、特定の入力を使い任意のトピックについて話すように操作できた結果、GPT3 のバックドアアクセスを提供してしまった。

> 再犯予測の事例：再犯のデータについて人種を含むものと含まないものを用意して被告人を記述する簡単なプロンプトを生成し大規模生成モデルに判断させたところ、モデルサイズが大きくなると全体的な予測能力は向上したが、同時に黒人の偽陽性判定率が急激に大きくなった（下図）。

![Alt text](<第2問図表/Screenshot 2023-08-22 at 21.51.52.png>)

> これは[COMPAS](https://www.axion.zone/is-the-recidivism-prediction-algorithm-fair-to-race/)という再犯の教師データセットが存在したために検出できた害であり、より一般には害があるのかどうかはやってみないとわからない。

2.4 オープンエンドの出力の問題点

入力が固定されていても出力は多様（ただし、出力の多様性はすでに多くの研究がある）。

- distractive: 入力と関係ない出力をする
- misleading: 誤った情報を出力する
- toxicity: 失礼な、不敬な、あるいは理不尽な用語を使う

![Alt text](<第2問図表/Screenshot 2023-08-22 at 21.54.39.png>)

LLM の多くの応用はオープンエンドなテキスト生成のため、

1. 正確性、安全性等の社会的に関連する側面を数量化すること
2. 出力の正確性を向上させる技術の開発(有害なバイアスの排除、適切な出力)

が重要

3.1 3 つの動機

経済性

スケーリング則により正確に開発コストを見積もることができる。開発期間が広範な能力と、特定の能力どちらの向上を意図していても同様に経済的インセンティブはある。

科学

大規模言語モデルは学際的な AI の研究の基盤になる。AI モデルへのアクセスがない場合その他の分野での進歩促進効果の研究は困難

名声

大規模モデルが可能性に満ちたフロンティアにあるということ自体が、開発する動機になる。大規模モデルは一般にわかりやすい優位性があり、熟練した AI 研究者の採用にも影響し、同じ組織が展開している大規模言語モデル以外の関係ないサービスの売り上げも伸ばしうる。

3.2 3 つの障壁

モデル拡大の財務・能力コスト、AI 安全性の課題、モデル配備の規格と規範の欠如

3.3 モデル開発の急速な進展、産業と学術の隔絶

モデルの拡大速度が大きく、産業と学術の隔絶が生じ、害や論争を引き起こしている。

- 大規模言語モデルは急速に広がっている
- 産業分野は学術分野と比較して資源集約型モデル開発の大部分を担当
- 大規模モデルの展開はすでに害や論争を引き起こしている。

**大規模言語モデルの増加**

GPT-3 公開の一年後くらいで世界中から類似のモデルの発表が続いた

![Alt text](<第2問図表/Screenshot 2023-08-22 at 21.54.50.png>)

**産業-学術の負担割合の変化**

無料で一般に公開されているモデルは産業分野で開発されているモデルよりもパラメータが１〜２桁小さい。

![Alt text](<第2問図表/Screenshot 2023-08-22 at 21.55.11.png>)

より広範囲かつ長期にわたる高計算能力が必要な研究が学術界から産業界に移行していくトレンドの一部。

> 10 年前に比べて AI 研究は 30 万倍増加しているのに対し、学術界から来ている成果が占める割合はこの十年で急減し、ほぼ 0％。

安全性に対する研究は依然として必要性が認識されているものの、財政コスト、エンジニアリング能力の限界から、課題を抱えている。
学術は純粋な知的好奇心から動いているため、私企業よりも多様な専門知を提供できるかもしれない。

**害や論争**

AI システムが検証される前に展開され、憎悪的な言葉を生成した例や、トレーニングデータから disinformation campaign を支援した事例。

> Tay の事例：若者とコミュニケーションをする twitter bot で、ユーザーからナチスを讃える言葉を繰り返すよう指示されるなどするうちに、一日たたないうちに差別主義的発言を頻発するようになってしまった

![Alt text](%E7%AC%AC2%E5%95%8F%E5%9B%B3%E8%A1%A8/tay-artificial-intelligence-twitter.png)

欧州委員会は AI 法案を提案、高リスクな AI システムの展開と監視の方法を標準化しようとしている。

4. 介入

潜在的な危険はあるが、今後の開発・展開されるだろう。より良い開発・展開される可能性を高めるための技術的、政策的介入について概説

- 計算非対称性をなくす：優秀な人材を保持できるような、財政的構造的投資。無料計算リソースとしての国立研究クラウドの開発と展開(cf. Compute Canada, National AI Research Task foruce)。
- 敵対的データセットによる継続的な評価：モデル開発者が内部で red teaming 法に投資し、技術、データセット、ポリシーの選択肢を発表あるいはサービスとして提供。バグバウンティイニシアチブ、自動化による支援
- ガバナンス構造の変化させ参入者の動機を変える：ソフト規制＝ボランティアベストプラクティスの作成、ハード規制＝ベストプラクティスの立法、規格化。監督機関によるガバナンス、AI システムの能力を測定するための取り組み支援、監査に焦点を当てたエコシステムの創造
- 新しい能力を測定するためのオープンソースツールとフレームワークの開発：funder は評価システムを構築し、批判する研究者に資金を割り当てるべき。(cf. Language Model Evaluation Harness, BIG-Bench, HuggingFace の BERTology)
- 能力の急激なジャンプに対する理解の向上：解釈性の研究＝トランスフォーマが実行する計算をリバースエンジニアリングする試み
